{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy and matplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras \n",
    "import keras as kr \n",
    "# this are need for export the model al proto for be used in c#\n",
    "from keras import backend as K\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image generator for argumented data\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility for split the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Images \n",
    "x = []\n",
    "with open(\"data/images.txt\") as f:\n",
    "    for line in f:\n",
    "      x.append(np.asarray([float(x) for x in line.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8298"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of images\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into np and reshape\n",
    "x = np.asarray(x)\n",
    "x = x.reshape(len(x),30,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to float32\n",
    "x = x.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x295abb1a088>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check one image\n",
    "plt.imshow(x[0].reshape(30,30), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "y = []\n",
    "with open(\"data/labels.txt\") as f:\n",
    "    for line in f:\n",
    "      y.append(int(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfor labels into np array\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into vectors of size 10\n",
    "y = kr.utils.to_categorical(y, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train\n",
    "# 33% would be the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=(30,30,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(32,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(3, 3),activation='relu'))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dense(units=456, activation='relu'))\n",
    "kr.layers.BatchNormalization()\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=13, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 456)               467400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 456)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                5941      \n",
      "=================================================================\n",
      "Total params: 538,333\n",
      "Trainable params: 538,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# need first and last layer name for use in c#\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image generator\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch = 10\n",
    "#history_callback = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = gen.flow(X_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, y_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "937/937 [==============================] - 12s 13ms/step - loss: 0.3300 - accuracy: 0.8926 - val_loss: 0.0201 - val_accuracy: 0.9963\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0234 - accuracy: 0.9924 - val_loss: 7.8921e-04 - val_accuracy: 0.9956\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 1.9528e-04 - val_accuracy: 0.9998\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 10s 10ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 2.9962e-05 - val_accuracy: 0.9995\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 1.5589e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 1.3936e-04 - val_accuracy: 0.9996\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 4.0124e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 1.2105e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 2.1371e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 1.2693e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x295b237fc48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train 10 epoch\n",
    "epoch =10\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 11s 12ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 1.3132e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 5.5507e-07 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.5646e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 7.8952e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 1.3877e-07 - val_accuracy: 0.9985\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 2.4680e-07 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 5.4971e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 8.4240e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.2483e-06 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x247c1434808>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze function from stack overflow\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-af0a42548ffb>:27: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 55 variables.\n",
      "INFO:tensorflow:Converted 55 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# froze the graph\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/gesture_model1.pb'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model in file\n",
    "tf.train.write_graph(frozen_graph, \"../model/\", \"gesture_model1.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247cef534c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANWUlEQVR4nO3dXYhcdZrH8d/PbKJ5MxrTxrxpT4LiamCTpQkLWcQwzODKQMzFhORiyIJscjHCCHOx4l6Md4pMMszFIsQ1TGZxnRl8wSC6OxIHZG7EVjIxpl0nK70zmYSkoyMaXxKTfvaiT4bets+psupUnTbP9wNFVZ3nvDw55Nenqv516jgiBODyd0XTDQDoD8IOJEHYgSQIO5AEYQeSIOxAEn/VzcK275L0U0mzJP1bRDxSNf+SJUticHCwm00CqDA6OqozZ854ulrHYbc9S9K/SvqWpOOSXrd9ICKOli0zODio4eHhTjcJoIWhoaHSWjcv4zdIOhYR70XEeUm/kLS5i/UB6KFuwr5C0h8nPT9eTAMwA3UT9uneF3zpu7e2d9oetj08NjbWxeYAdKObsB+XtGrS85WSTkydKSL2RsRQRAwNDAx0sTkA3egm7K9Lutn2N2zPkbRN0oF62gJQt44/jY+IC7bvk/Rfmhh62xcRb9fWGYBadTXOHhEvSnqxpl4A9BDfoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKKra73ZHpX0saSLki5ExFAdTQGoX1dhL2yKiDM1rAdAD/EyHkii27CHpF/bfsP2zjoaAtAb3b6M3xgRJ2xfL+ll2+9ExKuTZyj+COyUpBtvvLHLzQHoVFdH9og4UdyflvScpA3TzLM3IoYiYmhgYKCbzQHoQsdhtz3f9sJLjyV9W9KRuhoDUK9uXsYvlfSc7Uvr+Y+I+M9augJQu47DHhHvSfqbGnsB0EMMvQFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kEQdPziJKc6dO1da27JlS+Wy8+bNK609/fTTHfcEcGQHkiDsQBKEHUiCsANJEHYgCcIOJMHQWw9s3769tPbSSy9VLlv12/o7duworV133XWltT179lRuEzlwZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJFqOs9veJ+k7kk5HxNpi2mJJv5Q0KGlU0taI+HPv2vx62bp1a2ntlVdeqVx20aJFpbV33323tLZy5crS2sMPP1y5zcWLF5fWdu3aVVobHx+vXO/FixdLa7Nnzy6tff7556W1q666qnKbKNfOkf1nku6aMu0BSQcj4mZJB4vnAGawlmGPiFclfTBl8mZJ+4vH+yXdU3NfAGrW6Xv2pRFxUpKK++vLZrS90/aw7eGxsbEONwegWz3/gC4i9kbEUEQMVX3vG0BvdRr2U7aXSVJxf7q+lgD0QqdhPyDp0ilYOyQ9X087AHqlnaG3pyTdKWmJ7eOSfiTpEUm/sn2vpD9I+m4vm/y62bZtW2lt6dKllcsuXLiwtLZ79+7S2ty5c0trH374YeU2q4beqk7JXb58eeV6q4bQbr/99tLaO++8U1pbvXp15Tar/i3ZtQx7RJSdnP3NmnsB0EN8gw5IgrADSRB2IAnCDiRB2IEk+HXZPtu0aVPHyz766KOltRdeeKG0dvbs2cr1vv/++6W1zz77rLT2ySefVK636gy+gwcPltZmzZrVUT+StHHjxtLaFVfkPrbl/tcDiRB2IAnCDiRB2IEkCDuQBGEHkmDo7Wtk1apVpbXbbruttDYyMlK53tHR0dLasWPHSmtHjx6tXG/VGWhVF6K84447Sms33XRT5TZtV9Yz48gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzn6Z+PTTT0trVb/yKknnz5+vu52W66063bRqrHzevHld9ZQZR3YgCcIOJEHYgSQIO5AEYQeSIOxAEu1c2HGfpO9IOh0Ra4tpD0n6J0ljxWwPRsSLvWoSrVUNc7Uaevviiy9KaxcuXCittfq11qr6lVdeWVqbM2dO5XrRmXaO7D+TdNc0038SEeuKG0EHZriWYY+IVyV90IdeAPRQN+/Z77N92PY+29fW1hGAnug07I9JWiNpnaSTknaXzWh7p+1h28NjY2NlswHosY7CHhGnIuJiRIxLelzShop590bEUEQMDQwMdNongC51FHbbyyY93SLpSD3tAOiVdobenpJ0p6Qlto9L+pGkO22vkxSSRiXt6mGPaMO5c+dKa63OaqsaBps7d25preoCjJJ0zTXXlNZuueWW0trKlStLa1W9olrLsEfE9mkmP9GDXgD0EN+gA5Ig7EAShB1IgrADSRB2IAnCDiTBr8teJq6++urSWtV4t9T5Ka6LFi2qXO+tt95aWlu7dm1pbf78+aW12bNnV26Tq7iW48gOJEHYgSQIO5AEYQeSIOxAEoQdSIKht8vEpk2bSmtr1qypXHZ4eLi0VnXq7OrVqyvXWzX0VjVUWPXrsq1+0Rbl2HNAEoQdSIKwA0kQdiAJwg4kQdiBJBh6u0xU/erqggULKpddv359aa3qrLdWZ9MtXLiwtFZ19hrDa73BXgWSIOxAEoQdSIKwA0kQdiAJwg4k0c6FHVdJ+rmkGySNS9obET+1vVjSLyUNauLijlsj4s+9axWduuGGGyrrEVFaGx8fL60tWbKkcr1Vw2utLgqJ+rVzZL8g6YcR8deS/k7S923fJukBSQcj4mZJB4vnAGaolmGPiJMR8Wbx+GNJI5JWSNosaX8x235J9/SqSQDd+0rv2W0PSlov6TVJSyPipDTxB0HS9XU3B6A+bYfd9gJJz0i6PyI++grL7bQ9bHt4bGyskx4B1KCtsNuerYmgPxkRzxaTT9leVtSXSTo93bIRsTcihiJiaGBgoI6eAXSgZdg9cT2dJySNRMSeSaUDknYUj3dIer7+9gDUpZ2z3jZK+p6kt2wfKqY9KOkRSb+yfa+kP0j6bm9aBFCHlmGPiN9KKrta3jfrbQe90Opih8uXL+9TJ2gS36ADkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiinau4rrL9G9sjtt+2/YNi+kO2/2T7UHG7u/ftAuhUO1dxvSDphxHxpu2Fkt6w/XJR+0lE/Lh37QGoSztXcT0p6WTx+GPbI5JW9LoxAPX6Su/ZbQ9KWi/ptWLSfbYP295n+9qaewNQo7bDbnuBpGck3R8RH0l6TNIaSes0ceTfXbLcTtvDtofHxsZqaBlAJ9oKu+3Zmgj6kxHxrCRFxKmIuBgR45Iel7RhumUjYm9EDEXE0MDAQF19A/iK2vk03pKekDQSEXsmTV82abYtko7U3x6AurTzafxGSd+T9JbtQ8W0ByVtt71OUkgalbSrJx0CqEU7n8b/VpKnKb1YfzsAeoVv0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQcEf3bmD0m6X8nTVoi6UzfGmiNfqrNtH6kmddT0/3cFBED0xX6GvYvbdwejoihxhqYgn6qzbR+pJnX00zrZzJexgNJEHYgiabDvrfh7U9FP9VmWj/SzOtppvXzF42+ZwfQP00f2QH0SSNht32X7f+2fcz2A030MKWfUdtv2T5ke7ihHvbZPm37yKRpi22/bPv3xf21DffzkO0/FfvpkO27+9jPKtu/sT1i+23bPyimN7KPKvppbB+10veX8bZnSXpX0rckHZf0uqTtEXG0r438/55GJQ1FRGPjo7bvkHRW0s8jYm0x7VFJH0TEI8UfxWsj4p8b7OchSWcj4sf96GFKP8skLYuIN20vlPSGpHsk/aMa2EcV/WxVQ/uolSaO7BskHYuI9yLivKRfSNrcQB8zSkS8KumDKZM3S9pfPN6vif9MTfbTmIg4GRFvFo8/ljQiaYUa2kcV/cxYTYR9haQ/Tnp+XM3vpJD0a9tv2N7ZcC+TLY2Ik9LEfy5J1zfcjyTdZ/tw8TK/b28rJrM9KGm9pNc0A/bRlH6kGbCPptNE2D3NtKaHBDZGxN9K+gdJ3y9ewuLLHpO0RtI6SScl7e53A7YXSHpG0v0R8VG/t99GP43vozJNhP24pFWTnq+UdKKBPv4iIk4U96clPaeJtxozwaniveGl94inm2wmIk5FxMWIGJf0uPq8n2zP1kSwnoyIZ4vJje2j6fppeh9VaSLsr0u62fY3bM+RtE3SgQb6kCTZnl98wCLb8yV9W9KR6qX65oCkHcXjHZKeb7CXS2G6ZIv6uJ9sW9ITkkYiYs+kUiP7qKyfJvdRSxHR95ukuzXxifz/SPqXJnqY1MtqSb8rbm831Y+kpzTxsu8LTbz6uVfSdZIOSvp9cb+44X7+XdJbkg5rImTL+tjP32vi7d5hSYeK291N7aOKfhrbR61ufIMOSIJv0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/AFv+5Z9EFNKzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.imshow(x1[8].reshape(30,30), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
