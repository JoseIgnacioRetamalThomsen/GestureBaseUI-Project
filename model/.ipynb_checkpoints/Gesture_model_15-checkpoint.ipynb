{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy and matplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras \n",
    "import keras as kr \n",
    "# this are need for export the model al proto for be used in c#\n",
    "from keras import backend as K\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image generator for argumented data\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility for split the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Images \n",
    "x = []\n",
    "with open(\"data/images15.txt\") as f:\n",
    "    for line in f:\n",
    "      x.append(np.asarray([float(x) for x in line.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of images\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into np and reshape\n",
    "x = np.asarray(x)\n",
    "x = x.reshape(len(x),30,30,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to float32\n",
    "x = x.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16fe7066dc8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANL0lEQVR4nO3dXahd9ZnH8e9jEs27JpNjjKk2HV9nECYZDzrgMDiUFkcK6kWlXpQMyKQXFaoUHHGQ6p1IVXoxKHEMTQfHtqBiLmSmEgrSG/GoGV+acepIpk1zSI4oqOBbzDMXZ1nOxLPXPu73+nw/sNl7r2evvR5W8jtrr/3fa63ITCR98Z0y7gYkjYZhl4ow7FIRhl0qwrBLRRh2qYjl/cwcEVcBPwKWAf+SmXe3vX7Tpk25bdu2fhYpqcWhQ4d48803Y7Faz2GPiGXAPwNfAw4Dz0XEvsz8dad5tm3bxszMTK+LlNTF9PR0x1o/H+MvA17PzDcy8yPgp8A1fbyfpCHqJ+xbgd8teH64mSZpAvUT9sX2Cz7z29uI2BURMxExMzc318fiJPWjn7AfBs5Z8PxLwJGTX5SZuzNzOjOnp6am+licpH70E/bngAsi4isRcSrwLWDfYNqSNGg9fxufmccj4ibgP5gfetuTma8OrDNJA9XXOHtmPgU8NaBeJA2Rv6CTijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSqir2u9RcQh4F3gE+B4Zk4PoimNVkR0rGXmCDvRMPUV9sbfZuabA3gfSUPkx3ipiH7DnsAvIuL5iNg1iIYkDUe/H+OvyMwjEXEm8HRE/FdmPrPwBc0fgV0A5557bp+Lk9SrvrbsmXmkuT8GPAFctshrdmfmdGZOT01N9bM4SX3oOewRsSYi1n36GPg68MqgGpM0WP18jN8MPNEM2ywH/i0z/30gXUkauJ7DnplvAH8xwF40JOvXr2+tb9iwoWNty5YtHWuzs7M996TRc+hNKsKwS0UYdqkIwy4VYdilIgy7VMQgjnrTiLT9AnH58s7/lOvWrWt932XLlnWsrVq1qmPtoosuan3f1157rbWu0XLLLhVh2KUiDLtUhGGXijDsUhGGXSrCobc/Im1Hr7UNvZ1yynD+prcN2QHs2LGjY+3FF18cdDvqwi27VIRhl4ow7FIRhl0qwrBLRRh2qQiH3ibMpZde2rG2cePGnt6z2xBZ24Ud+7FmzZqhvK9645ZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4roOs4eEXuAbwDHMvOSZtpG4GfANuAQcH1mvj28NutYvXp1x9ppp53W03ueeuqpvbbT6qOPPmqtO84+WZayZf8xcNVJ024D9mfmBcD+5rmkCdY17Jn5DPDWSZOvAfY2j/cC1w64L0kD1us+++bMnAVo7s/s9MKI2BURMxExMzc31+PiJPVr6F/QZebuzJzOzOm2K5pIGq5ew340IrYANPfHBteSpGHoNez7gJ3N453Ak4NpR9KwLGXo7VHgSmBTRBwGfgDcDfw8Im4Efgt8c5hNVtJ2EcaVK1d2rLWdQbbtzLP96HbW2rVr1w5luepN1/8FmXlDh9JXB9yLpCHyF3RSEYZdKsKwS0UYdqkIwy4V4dllJ0zbxRvbjnpbsWJFTzXofvbZXm3durVj7Z577ulYu/XWW4fRTnlu2aUiDLtUhGGXijDsUhGGXSrCsEtFOPQ2YU4//fSOtbYhtH6Oeuv1RJbd5mu7YGTbvPfff3/H2i233NK9MS3KLbtUhGGXijDsUhGGXSrCsEtFGHapCMMuFeE4+4Q544wzepqv7TDVbhd2bBsPbxu/73b22LZ533///Y61s88+u/V91Ru37FIRhl0qwrBLRRh2qQjDLhVh2KUilnJhxz3AN4BjmXlJM+1O4B+AueZlt2fmU8Nq8ovkrrvuaq2vXr26p/ddtWpVx1q3s8u2HQLbz1lr2963bdhu06ZNre+r3ixly/5j4KpFpt+fmdubm0GXJlzXsGfmM8BbI+hF0hD1s89+U0S8FBF7ImLDwDqSNBS9hv0B4DxgOzAL3NvphRGxKyJmImJmbm6u08skDVlPYc/Mo5n5SWaeAB4CLmt57e7MnM7M6ampqV77lNSnnsIeEVsWPL0OeGUw7UgalqUMvT0KXAlsiojDwA+AKyNiO5DAIeA7Q+zxC6XtSDBoH85qG15rq/VzFti2frsNvbXN29bv+eef3/q+6k3XsGfmDYtMfngIvUgaIn9BJxVh2KUiDLtUhGGXijDsUhGGXSrCs8uO2B133NFaf/DBBzvW2sam2w6NbZuvm48//rjnedvG99evX9+xtnnz5p6Xqc7csktFGHapCMMuFWHYpSIMu1SEYZeKcOhtwrQNOx0/frxjbc2aNR1rbcNc3bRdgPHDDz9snbftENitW7d2rHU7JFe9ccsuFWHYpSIMu1SEYZeKMOxSEYZdKsKhtwlz+eWXd6wdOXKkY+2DDz7oWOv1YpEAy5Yt61jrdnbZs846q2Ot7Ui8lStXdm9Mn5tbdqkIwy4VYdilIgy7VIRhl4ow7FIRS7mw4znAT4CzgBPA7sz8UURsBH4GbGP+4o7XZ+bbw2u1hvfee69j7eKLL+5Ye/vtzqv+xIkTrctsuwBj29Bbt6Ppli/v/N+r7Qg+DcdStuzHge9n5p8BfwV8NyL+HLgN2J+ZFwD7m+eSJlTXsGfmbGa+0Dx+FzgIbAWuAfY2L9sLXDusJiX173Pts0fENmAH8CywOTNnYf4PAnDmoJuTNDhLDntErAUeA27OzHc+x3y7ImImImbm5uZ66VHSACwp7BGxgvmgP5KZjzeTj0bElqa+BTi22LyZuTszpzNzempqahA9S+pB17BHRAAPAwcz874FpX3AzubxTuDJwbcnaVCWctTbFcC3gZcj4kAz7XbgbuDnEXEj8Fvgm8NpUdIgdA17Zv4KiA7lrw62HV144YUda22HsU7iLpKHqk4Wf0EnFWHYpSIMu1SEYZeKMOxSEYZdKsKzy/4RcShL/XDLLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4pYylVcz4mIX0bEwYh4NSK+10y/MyJ+HxEHmtvVw29XUq+WcnbZ48D3M/OFiFgHPB8RTze1+zPzh8NrT9KgLOUqrrPAbPP43Yg4CGwddmOSButz7bNHxDZgB/BsM+mmiHgpIvZExIYB9yZpgJYc9ohYCzwG3JyZ7wAPAOcB25nf8t/bYb5dETETETNzc3MDaFlSL5YU9ohYwXzQH8nMxwEy82hmfpKZJ4CHgMsWmzczd2fmdGZOT01NDapvSZ/TUr6ND+Bh4GBm3rdg+pYFL7sOeGXw7UkalKV8G38F8G3g5Yg40Ey7HbghIrYDCRwCvjOUDiUNxFK+jf8VEIuUnhp8O5KGxV/QSUUYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhGGXSrCsEtFGHapCMMuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIiIzR7ewiDngfxdM2gS8ObIGurOfdpPWD0xeT+Pu58uZObVYYaRh/8zCI2Yyc3psDZzEftpNWj8weT1NWj8L+TFeKsKwS0WMO+y7x7z8k9lPu0nrByavp0nr5w/Gus8uaXTGvWWXNCJjCXtEXBURr0XE6xFx2zh6OKmfQxHxckQciIiZMfWwJyKORcQrC6ZtjIinI+I3zf2GMfdzZ0T8vllPByLi6hH2c05E/DIiDkbEqxHxvWb6WNZRSz9jW0fdjPxjfEQsA/4b+BpwGHgOuCEzfz3SRv5/T4eA6cwc2/hoRPwN8B7wk8y8pJl2D/BWZt7d/FHckJn/OMZ+7gTey8wfjqKHk/rZAmzJzBciYh3wPHAt8PeMYR219HM9Y1pH3Yxjy34Z8HpmvpGZHwE/Ba4ZQx8TJTOfAd46afI1wN7m8V7m/zONs5+xyczZzHyhefwucBDYypjWUUs/E2scYd8K/G7B88OMfyUl8IuIeD4ido25l4U2Z+YszP/nAs4ccz8AN0XES83H/JHtViwUEduAHcCzTMA6OqkfmIB1tJhxhD0WmTbuIYErMvMvgb8Dvtt8hNVnPQCcB2wHZoF7R91ARKwFHgNuzsx3Rr38JfQz9nXUyTjCfhg4Z8HzLwFHxtDHH2Tmkeb+GPAE87sak+Bos2/46T7isXE2k5lHM/OTzDwBPMSI11NErGA+WI9k5uPN5LGto8X6Gfc6ajOOsD8HXBARX4mIU4FvAfvG0AcAEbGm+YKFiFgDfB14pX2ukdkH7Gwe7wSeHGMvn4bpU9cxwvUUEQE8DBzMzPsWlMayjjr1M8511FVmjvwGXM38N/L/A/zTOHpY0MufAv/Z3F4dVz/Ao8x/7PuY+U8/NwJ/AuwHftPcbxxzP/8KvAy8xHzItoywn79mfnfvJeBAc7t6XOuopZ+xraNuN39BJxXhL+ikIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhXxf27npEjx5hw/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check one image\n",
    "plt.imshow(x[1554].reshape(30,30), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labels\n",
    "y = []\n",
    "with open(\"data/labels15.txt\") as f:\n",
    "    for line in f:\n",
    "      y.append(int(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfor labels into np array\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into vectors of size 10\n",
    "y = kr.utils.to_categorical(y, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train\n",
    "# 33% would be the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "model = kr.models.Sequential()\n",
    "model.add(kr.layers.Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=(30,30,1)))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(32,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(3, 3),activation='relu'))\n",
    "kr.layers.BatchNormalization(axis=-1)\n",
    "model.add(kr.layers.Conv2D(64,kernel_size=(3, 3),activation='relu'))\n",
    "model.add(kr.layers.MaxPooling2D(pool_size=(2, 2),))\n",
    "\n",
    "model.add(kr.layers.Flatten())\n",
    "model.add(kr.layers.Dense(units=456, activation='relu'))\n",
    "kr.layers.BatchNormalization()\n",
    "model.add(kr.layers.Dropout(0.1))\n",
    "model.add(kr.layers.Dense(units=15, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 456)               467400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 456)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                6855      \n",
      "=================================================================\n",
      "Total params: 539,247\n",
      "Trainable params: 539,247\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# need first and last layer name for use in c#\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image generator\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "test_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch = 10\n",
    "#history_callback = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=epoch, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = gen.flow(X_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, y_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "743/937 [======================>.......] - ETA: 2s - loss: 0.2731 - accuracy: 0.9146"
     ]
    }
   ],
   "source": [
    "#train 10 epoch\n",
    "epoch =10\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 11s 12ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 1.3132e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 5.5507e-07 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.5646e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 7.8952e-05 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 1.3877e-07 - val_accuracy: 0.9985\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 2.4680e-07 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 5.4971e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 11s 11ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 8.4240e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 10s 11ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.2483e-06 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x247c1434808>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=epoch, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze function from stack overflow\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-af0a42548ffb>:27: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From C:\\Users\\pepe\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 55 variables.\n",
      "INFO:tensorflow:Converted 55 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# froze the graph\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/gesture_model.pb'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model in file\n",
    "tf.train.write_graph(frozen_graph, \"../model/\", \"gesture_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247cef534c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.imshow(x1[8].reshape(30,30), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
